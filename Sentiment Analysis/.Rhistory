install.packages(c("devtools", "plotly"))
knitr::opts_chunk$set(echo = TRUE)
xx<- read.csv("People&Blogs.csv")
library(devtools)
if(!require(Rstem)) install_url("http://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz")
if(!require(sentiment)) install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
library(Rstem)
xx<- read.csv("People&Blogs.csv")
zaz<- xx$comment
zaz<- na.omit(zaz)
clean_tweets<-zaz
# using sentiment package to classify emotions
emotions <- classify_emotion(clean_tweets, algorithm='bayes')
library(devtools)
if(!require(Rstem)) install_url("http://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz")
if(!require(sentiment)) install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
library(Rstem)
library(sentiment)
library(plotly)
library(dplyr)
library(wordcloud)
if(!require(Rstem)) install_url("http://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz")
if(!require(sentiment)) install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
install.packages("tm")
install.packages("sentiment")
if(!require(sentiment)) install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
# # Install
# install.packages("tm")  # for text mining
# install.packages("SnowballC") # for text stemming
# install.packages("wordcloud") # word-cloud generator
# install.packages("RColorBrewer") # color palettes
# install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
# library(tm)
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
text <- readLines(file.choose())
text <- readLines(file.choose())
# Load the data as a corpus
#to remove emojis
text <- iconv(text, 'UTF-8', 'ASCII')
docs <- Corpus(VectorSource(text))
#inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "http")
docs <- tm_map(docs, toSpace, "www")
docs <- tm_map(docs, toSpace, "com")
docs <- tm_map(docs, toSpace, "udu")
docs <- tm_map(docs, toSpace, "udu")
docs <- tm_map(docs, toSpace, "daud")
docs <- tm_map(docs, toSpace, "udc")
docs <- tm_map(docs, toSpace, "bit.ly")
docs <- tm_map(docs, toSpace, "will")
docs <- tm_map(docs, toSpace, "dudca")
docs <- tm_map(docs, toSpace, "utm")
docs <- tm_map(docs, toSpace, "like")
docs <- tm_map(docs, toSpace, "names")
docs <- tm_map(docs, toSpace, "sewingpartsonline")
docs <- tm_map(docs, toSpace, "srilankaelle")
docs <- tm_map(docs, toSpace, "name")
docs <- tm_map(docs, toSpace, "can")
docs <- tm_map(docs, toSpace, "gmail")
docs <- tm_map(docs, toSpace, "new")
docs <- tm_map(docs, toSpace, "smartphone")
docs <- tm_map(docs, toSpace, "video")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "animation", corlimit = 0.3)
head(d, 10)
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
col ="green", main ="Most frequent words",
ylab = "Word frequencies")
x<-read.csv("commentsUK.csv")
View(x)
za<-x$comment_text
write.csv(za,"za.csv")
text <- readLines(file.choose())
# Load the data as a corpus
#to remove emojis
text <- iconv(text, 'UTF-8', 'ASCII')
docs <- Corpus(VectorSource(text))
#inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "http")
docs <- tm_map(docs, toSpace, "www")
docs <- tm_map(docs, toSpace, "com")
docs <- tm_map(docs, toSpace, "udu")
docs <- tm_map(docs, toSpace, "udu")
docs <- tm_map(docs, toSpace, "daud")
docs <- tm_map(docs, toSpace, "udc")
docs <- tm_map(docs, toSpace, "bit.ly")
docs <- tm_map(docs, toSpace, "will")
docs <- tm_map(docs, toSpace, "dudca")
docs <- tm_map(docs, toSpace, "utm")
docs <- tm_map(docs, toSpace, "like")
docs <- tm_map(docs, toSpace, "names")
docs <- tm_map(docs, toSpace, "sewingpartsonline")
docs <- tm_map(docs, toSpace, "srilankaelle")
docs <- tm_map(docs, toSpace, "name")
docs <- tm_map(docs, toSpace, "can")
docs <- tm_map(docs, toSpace, "gmail")
docs <- tm_map(docs, toSpace, "new")
docs <- tm_map(docs, toSpace, "smartphone")
docs <- tm_map(docs, toSpace, "video")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
rm(large_df, large_list, large_vector, temp_variables)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
rm(large_df, large_list, large_vector, temp_variables)
dtm <- TermDocumentMatrix(docs)
x<-read.csv("commentsUS.csv")
za<-x$comment_text
write.csv(za,"za.csv")
